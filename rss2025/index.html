<!DOCTYPE html>
<html lang="en" xmlns="">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <script type="text/javascript"
          src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>3rd Workshop on Dexterous Manipulation: Learning and Control with Diverse Data</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>
<div class="nav">
  <div class="nav-container">
    <a href="https://dex-manipulation.github.io/rss2024/index.html#intro">Introduction</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#speaker">Speakers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#call">Call for Papers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#schedule">Schedule</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#organizers">Organizers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#contact">Contact</a>
  </div>
</div>

<div class="title-container">
  <div style="text-align: center;">
    <h1>3rd Workshop on Dexterous Manipulation: <br>Learning and Control with Diverse Data</h1>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      RSS 2025 Workshop Proposal, June 21/25, 2025
    </div>
  </div>
</div>

<div class="container">
  <div class="section" id="intro">
    <h2>Introduction</h2>
    <p>
      Dexterous manipulation is one of the hardest problems in robotics. Despite decades of research, robots still struggle with tasks that human hands perform effortlessly. Solving this challenge requires breakthroughs in multiple fields—mechanical design, motion planning, control, and machine learning. This workshop brings together experts across these disciplines to explore what’s missing and what’s next.
    </p>
    <p>
      We invite researchers working on dexterous robotic manipulation, manipulator design, tactile sensing, visual perception, and human hand understanding from video. Attendees will have the opportunity to participate in discussions, present posters, and give contributed talks. Panelists and speakers, drawn from these research communities, include leading experts from both academia and industry.
    </p>
  </div>


  <div class="section" id="speakers">
    <h2>Invited Speakers</h2>
    <div class="people">
      <a href="https://ankurhanda.github.io/">
        <img src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/11/ankur-handa-270x270.png">
        <div>Ankur Handa</div>
        <div class="aff">NVIDIA</div>
      </a>

      <a href="https://tml.stanford.edu/">
        <img src="https://profiles.stanford.edu/proxy/api/cap/profiles/211555/resources/profilephoto/350x350.1566448250851.jpg">
        <div>Karen Liu</div>
        <div class="aff">Stanford University</div>
      </a>

      <a href="https://www.lerrelpinto.com/">
        <img src="https://www.lerrelpinto.com/authors/admin/avatar_huc8388dc171e86880932174789f883845_636300_1000x1000_fill_q75_lanczos_center.jpg">
        <div>Lerrel Pinto</div>
        <div class="aff">New York University</div>
      </a>

      <a href="https://www.cs.cmu.edu/~nsp/">
        <img src="http://graphics.cs.cmu.edu/nsp/nsp.jpg">
        <div>Nancy Polland</div>
        <div class="aff">Carnegie Mellon University</div>
      </a>

      <a href="https://ait.ethz.ch/people/song">
        <img src="https://ait.ethz.ch/assets/people/jie.jpg">
        <div>Jie Song</div>
        <div class="aff">ETH Zurich</div>
      </a>

      <a href="https://hapticsensinglab.com/">
        <img src="https://hapticsensinglab.com/People/b21809ad765b4c3e68e62981f16ad128.jpg">
        <div>Huanbo Sun</div>
        <div class="aff">Peking University</div>
      </a>

      <a href="https://locomotion.csail.mit.edu/russt.html">
        <img src="https://www.tri.global/sites/default/files/styles/bio/public/2024-10/Russ_Tedrake.jpg?itok=cY24lG9a">
        <div>Russ Tedrake
        </div>
        <div class="aff">MIT</div>
      </a>

    </div>
    <p> (<small><i>listed alphabetically</i></small>) </p>
  </div>
  <br>

  <div class="section" id="schedule">
    <h2>Draft Workshop Schedule</h2>
    <ul></ul>
    <style type="text/css">
        .tg .tg-u4qn {
            background-color: #D9D9D9;
            text-align: left;
            vertical-align: bottom
        }
    </style>
    <table>
      <tr>
        <th>Time (UTC -8)</th>
        <th>Event</th>
      </tr>
      <tr>
        <td>08:55 - 09:00</td>
        <td>Introduction and Opening Remark</td>
      </tr>
      <tr>
        <td>09:00 - 09:30</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>09:30 - 10:00</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>10:00 - 10:30</td>
        <td>Spotlight Session 1</td>
      </tr>
      <tr>
        <td>10:30 - 11:30</td>
        <td>Poster / Demo Session</td>
      </tr>
      <tr>
        <td>10:30 - 10:55</td>
        <td>Poster Session + Real Robot Demo Session</td>
      </tr>
      <tr>
        <td>11:30 - 12:00</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>12:00 - 12:30</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>14:00 - 14:30</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>14:30 - 15:00</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>15:00 - 15:30</td>
        <td>Invited Talk</td>
      </tr>
      <tr>
        <td>15:30 - 16:00</td>
        <td>Spotlight Session 2</td>
      </tr>
      <tr>
        <td>16:00 - 17:00</td>
        <td>Poster Session + Real Robot Demo Session</td>
      </tr>
      <tr>
        <td>17:00 - 17:55</td>
        <td>Panel Discussion</td>
      </tr>
      <tr>
        <td>17:55 - 18:00</td>
        <td>Closing Remark & Award Ceremony</td>
      </tr>
    </table>
    <ul></ul>
  </div>
  <br>

<!--  <div class="section" id="call">-->
<!--    <h2>Call for Papers/Demos</h2>-->
<!--    <p>-->
<!--      In this workshop, our goal is to bring together researchers from various fields of robotics, such as control,-->
<!--      optimization, learning, planning, sensing, hardware, etc., who work on dexterous manipulation.-->
<!--    </p>-->

<!--    <p>-->
<!--      We are particularly excited to offer a platform for showcasing real-world robotic systems.-->
<!--      <b style="color: darkblue">Even without a formal paper, we encourage submissions of videos demonstrating your-->
<!--        robots in action. For those-->
<!--        able to attend in person, there will be opportunities to showcase your robots live at the workshop!</b>-->
<!--      We encourage-->
<!--      researchers to submit work in the following areas (the list is not exhaustive):-->
<!--    </p>-->

<!--    <ul>-->
<!--      <li><strong>Learning for Dexterous Manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>Can we expect a foundation policy for most daily dexterous manipulation tasks, or will&ndash;&gt;-->
<!--      &lt;!&ndash;          each task require its own policy?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How can learning-based policies handle dynamic tasks that require high-frequency control?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>&ndash;&gt;-->
<!--      &lt;!&ndash;          How can we improve the generalization capability or the sample efficiency of policy learning in dexterous&ndash;&gt;-->
<!--      &lt;!&ndash;          manipulation?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->
<!--      <li><strong>Planning and Optimization in Dexterous Manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How to scale up planning/optimization methods to high DoF systems?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>&ndash;&gt;-->
<!--      &lt;!&ndash;          How to leverage both planning/optimization methods and learning methods to get the best of both worlds?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How can we make dexterous hands compliant?</li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->
<!--      <li><strong>Simulation for contact-rich manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How to better model the contacts between the hand and object?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How can we speed up simulation in the presence of frequent contacts?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->
<!--      <li><strong>Hardware design for Dexterous Manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>Is it possible to build a low-cost dexterous hand?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>What kind of hand design can make manipulation easier?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>&ndash;&gt;-->
<!--      &lt;!&ndash;          What benefits do soft hands bring?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>&ndash;&gt;-->
<!--      &lt;!&ndash;          Novel mechanisms that improve robotic hand performance.&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->
<!--      <li><strong>Data collection in Dexterous Manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How do we leverage human hand data in the wild to improve policy learning in dexterous manipulation?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How to develop low-cost, easy-to-use teleoperation systems for large-scale demonstration data?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>How do we leverage planning methods to generate large-scale demonstration data in simulation?</li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->
<!--      <li><strong>Tactile Sensing for Manipulation</strong></li>-->
<!--      &lt;!&ndash;      <ul>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>What's the role of tactile sensing for dexterous manipulation?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;        <li>&ndash;&gt;-->
<!--      &lt;!&ndash;          How to use tactile data efficiently for control or policy learning?&ndash;&gt;-->
<!--      &lt;!&ndash;        </li>&ndash;&gt;-->
<!--      &lt;!&ndash;      </ul>&ndash;&gt;-->

<!--      <li>Any additional related topics not already covered in the above list 😄</li>-->
<!--    </ul>-->
<!--    <h3>Submission Guidelines</h3>-->
<!--    <p>-->
<!--    <ul>-->
<!--      <li><b style="color: darkblue">Submission Portal for Paper and Demo:</b> <a-->
<!--          href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2024/Workshop/DM"-->
<!--          target="_blank">OpenReview</a>-->
<!--      </li>-->
<!--      <li><b style="color: darkblue">Paper Submission Guideline:</b></li>-->
<!--      <ul>-->
<!--        <li><b>Paper Length:</b>-->
<!--          Submissions could be <b>4-page</b> short papers, excluding references, acknowledgements, and appendices.-->
<!--        </li>-->
<!--        <li><b>Format:</b>-->
<!--          <ul>-->
<!--            <li>The format requirement follows the RSS 2024 main conference. Paper template: <a-->
<!--                href="https://roboticsconference.org/docs/paper-template-latex.tar.gz" target="_blank">LaTeX</a> and <a-->
<!--                href="https://roboticsconference.org/docs/paper-template-word.zip" target="_blank">Word</a></li>-->
<!--            <li>Please include the references and appendix in the same PDF as the main paper and submit optional videos-->
<!--              as zip file in supplementary.-->
<!--            </li>-->
<!--            <li>The maximum file size is 100MB. More information on-->
<!--              format can be found <a-->
<!--                  href="https://roboticsconference.org/information/authorinfo/#paper-and-demo-format"-->
<!--                  target="_blank">here</a>-->
<!--            </li>-->
<!--          </ul>-->
<!--        </li>-->
<!--        <li><b>Dual Submission for Other Conference:</b>-->
<!--          <ul>-->
<!--            <li>Papers to be submitted or in preparation for submission to other major venues (including CoRL 2024) in-->
<!--              the-->
<!--              field are allowed.-->
<!--            </li>-->
<!--            <li>We also welcome published works as long as explicitly stated at the time of submission.</li>-->
<!--          </ul>-->
<!--        </li>-->
<!--      </ul>-->

<!--      <li><b style="color: darkblue">Demo Submission Guideline:</b></li>-->
<!--      <ul>-->
<!--        <li><b>Summary Document:</b>-->
<!--          Please upload a summary document as a pdf. There is no page and format requirement for this pdf.-->
<!--        </li>-->
<!--        <li><b>Demo Video:</b>-->
<!--          Please upload video demos as a single zip under the supplementary material field in OpenReview.-->
<!--        </li>-->
<!--      </ul>-->
<!--      <li><b style="color: darkblue">Visibility:</b> Submissions and reviews will not be public. Only accepted papers-->
<!--        will be made public.-->
<!--      </li>-->
<!--    </ul>-->

<!--    <h3>Timeline</h3>-->
<!--    <ul>-->
<!--      <li>Submission Port Open: <b>Monday, April 15, 2024</b></li>-->
<!--      <li>Submission Deadline:-->
<!--        <del>Monday, June 10, 2024</del>-->
<!--        &nbsp <b style="color: red;">Friday, June 14, 2024</b></li>-->
<!--      <li>Notification: <b>Monday, July 1, 2024</b></li>-->
<!--      <li>Workshop Date: <b>Monday, July 15, 2024</b></li>-->
<!--    </ul>-->
<!--  </div>-->

<!--  <br>-->

<!--  <div class="section" id="postersessions">-->
<!--    <h2>Poster Sessions</h2>-->
<!--    &lt;!&ndash;    <p>Poster session assignments are posted below. The session will be held at .&ndash;&gt;-->

<!--    <table style="width: 100%">-->
<!--      <thead>-->
<!--      <tr>-->
<!--        <th></th>-->
<!--        <th>Spotlight Session I (09:40-10:10)</th>-->
<!--        <th></th>-->
<!--        <th>Spotlight Session II (15:15-15:35)</th>-->
<!--      </tr>-->
<!--      </thead>-->
<!--      <tbody>-->
<!--      <tr>-->
<!--        <td>Paper Name</td>-->
<!--        <td>Authors</td>-->
<!--        <td>Paper Name</td>-->
<!--        <td>Authors</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=KwwJuZIBXH'>3D Diffusion Policy: Generalizable Visuomotor Policy-->
<!--          Learning via Simple 3D Representations</a>-->
<!--        </td>-->
<!--        <td>Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu</td>-->
<!--        <td><a href='https://openreview.net/forum?id=n2K5OWLf7C'>SpringGrasp: Synthesizing Compliant, Dexterous Grasps-->
<!--          under Shape Uncertainty-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Sirui Chen, Jeannette Bohg, Karen Liu</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=EiarCRjOm7'>DexCap: Scalable and Portable Mocap Data Collection-->
<!--          System for Dexterous Manipulation</a>-->
<!--        </td>-->
<!--        <td>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, Karen Liu</td>-->
<!--        <td><a href='https://openreview.net/forum?id=EvapfdGjfY'>Tilde: Teleoperation for Dexterous In-Hand Manipulation-->
<!--          Learning with a DeltaHand-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Zilin Si, Kevin Lee Zhang, Zeynep Temel, Oliver Kroemer</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=d8qYLDH2vj'>ContactMPC: Towards Online Adaptive Control for-->
<!--          Contact-Rich Dexterous Manipulation</a>-->
<!--        </td>-->
<!--        <td>Arjun Lakshmipathy, Nancy S. Pollard</td>-->
<!--        <td><a href='https://openreview.net/forum?id=V6XHKFicwc'>DexGraspNet 2.0: Learning Generative Dexterous Grasping-->
<!--          in Large-scale Synthetic Cluttered Scenes-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Jialiang Zhang, Haoran Liu, Danshi Li, Xinqiang Yu, Haoran Geng, Yufei Ding, Jiayi Chen, He Wang</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=7sKUwZvTD6'>Tactile Exploration with Particle-Based Belief-->
<!--          Entropy</a>-->
<!--        </td>-->
<!--        <td>Lara Brudermüller, Julius Jankowski, Sylvain Calinon, Marc Toussaint, Nick Hawes</td>-->
<!--        <td><a href='https://openreview.net/forum?id=3deVq5EoIZ'>Next-Gen Manipulation: An Active Surface-Based-->
<!--          Underactuated Gripper with Visual Feedback-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Karthik Swaminathan, Saad Hashmi, Alqama Shaikh, Vikas Phalle</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=pqgoIGvcgY'>RP1M: A Large-Scale Motion Dataset for Piano Playing-->
<!--          with Bi-Manual Dexterous Robot Hands</a>-->
<!--        </td>-->
<!--        <td>Le Chen, Yi Zhao, Jan Schneider, Quankai Gao, Juho Kannala, Bernhard Schölkopf, Joni Pajarinen, Dieter-->
<!--          Büchler-->
<!--        </td>-->
<!--        <td><a href='https://openreview.net/forum?id=mZGi8ANPxz'>PianoMime: Learning a Generalist, Dexterous Piano-->
<!--          Player from Internet Demonstrations-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Cheng Qian, Julen Urain, Kevin Zakka, Jan Peters</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=eQomRzRZEP'>LEAP Hand V2: Dexterous, Low-cost Anthropomorphic-->
<!--          Hybrid Rigid Soft Hand for Robot Learning</a>-->
<!--        </td>-->
<!--        <td>Kenneth Shaw, Deepak Pathak</td>-->
<!--        <td><a href='https://openreview.net/forum?id=2uDOjKQG85'>Seahorse Tail-inspired Soft Pneumatic Actuator-->
<!--          Utilizing Dual-mode Actuation-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Dickson Chiu Yu Wong, Zijian Zhou, Garmisch Lai Yin Wong, Rob B.N. Scharff</td>-->
<!--      </tr>-->
<!--      </tbody>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=iO0lDp0fSy'>OmniH2O: Universal and Dexterous Human-to-Humanoid-->
<!--          Whole-Body Teleoperation and Learning</a>-->
<!--        </td>-->
<!--        <td>Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris M. Kitani, Changliu Liu,-->
<!--          Guanya Shi-->
<!--        </td>-->
<!--        <td><a href='https://openreview.net/forum?id=edVs2ht61C'>A Dexterous Robotic Hand for In-Hand Manipulation of-->
<!--          Long, Thin Objects-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Abdullah Nazir</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=paxFR7SIHs'>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual-->
<!--          Dexterous Grasping and Articulation</a>-->
<!--        </td>-->
<!--        <td>Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges</td>-->
<!--        <td><a href='https://openreview.net/forum?id=Jy61QPqTUu'>CyberDemo: Augmenting Simulated Human Demonstration for-->
<!--          Real-World Dexterous Manipulation-->
<!--        </a>-->
<!--        </td>-->
<!--        <td>Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=YyBZdkD6ZQ'>A Compliant Gripper System for Delicate Object Grasping-->
<!--          through Intrinsic Contact Sensing</a>-->
<!--        </td>-->
<!--        <td>Tao Yu, Jiepeng Wang, Shujie Tang, Xingyue Zhu, Genliang Chen</td>-->
<!--      </tr>-->

<!--      <tr>-->
<!--        <td><a href='https://openreview.net/forum?id=LnFqv4F8bl'>Sarcomere Dynamics - High Dexterity Robotic Hands</a>-->
<!--        </td>-->
<!--        <td>Harpal Mandaher</td>-->
<!--      </tr>-->

<!--    </table>-->
<!--  </div>-->
<!--  <br>-->

<!--  <div class="section" id="award">-->
<!--    <h2>Awards and Recognition</h2>-->
<!--    <br>-->

<!--    <h3 style="font-size: 1.4em">Best Paper Award</h3>-->
<!--    <p style="font-size: 1.2em">-->
<!--      <strong>-->
<!--        Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand-->
<!--      </strong><br>-->
<!--      Zilin Si, Kevin Lee Zhang, Zeynep Temel, Oliver Kroemer-->
<!--    </p>-->

<!--    <br>-->
<!--    <h3 style="font-size: 1.4em">Best Presentation Award</h3>-->
<!--    <p style="font-size: 1.2em">-->
<!--      <strong>-->
<!--        LEAP Hand V2: Dexterous, Low-cost Anthropomorphic Hybrid Rigid Soft Hand for Robot Learning-->
<!--      </strong><br>-->
<!--      Kenneth Shaw, Deepak Pathak-->
<!--    </p>-->
<!--    <br>-->

<!--  </div>-->

  <div class="section" id="organizers">
    <h2>Organizers</h2>
    <div class="people">
      <a href="https://haozhi.io/">
        <img src="https://haozhi.io/profile.jpg">
        <div>Haozhi Qi</div>
        <div class="aff">UC Berkeley</div>
      </a>
      <a href="https://kennyshaw.net/">
        <img src="https://kennyshaw.net/images/profile.jfif">
        <div>Kenneth Shaw</div>
        <div class="aff">Carnegie Mellon University</div>
      </a>
      <a href="https://taochenshh.github.io/">
        <img src="https://taochenshh.github.io/media/tao_profile.jpg">
        <div>Tao Chen</div>
        <div class="aff">MIT</div>
      </a>
      <a href="https://yzqin.github.io/">
        <img src="https://yzqin.github.io/file/qyz_circle.png">
        <div>Yuzhe Qin</div>
        <div class="aff">UC San Diego</div>
      </a>
      <a href="https://irmakguzey.github.io/">
        <img src="https://irmakguzey.github.io/assets/img/profile_photo.jpg">
        <div>Irmak Guzey</div>
        <div class="aff">New York University</div>
      </a>
      <a href="https://irmakguzey.github.io/">
        <img src="https://www.ias.informatik.tu-darmstadt.de/uploads/Team/GeorgiaChalvatzaki/profile_geo.jpg">
        <div>Georgia Chalvatzaki</div>
        <div class="aff">TU Darmstadt</div>
      </a>
      <a href="https://jhugestar.github.io/">
        <img src="https://jhugestar.github.io/img/han_dec_2017.jpg">
        <div>Hanbyul Joo</div>
        <div class="aff">Seoul National University</div>
      </a>
      <a href="https://xiaolonw.github.io/">
        <img src="https://xiaolonw.github.io/static/profile.jpg">
        <div>Xiaolong Wang</div>
        <div class="aff">UC San Diego</div>
      </a>
      <a href="https://people.csail.mit.edu/pulkitag/">
        <img src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg">
        <div>Pulkit Agrawal</div>
        <div class="aff">MIT</div>
      </a>
    </div>
  </div>

<!--  <div class="section" id="sponsor">-->
<!--    <h2>Sponsor</h2>-->
<!--    <div>-->
<!--      <img src="images/dexmate.jpg" style="display: block; height: 120px " alt="DEXMATE">-->
<!--    </div>-->
<!--  </div>-->

  <div class="section" id="contact">
    <h2>Contact</h2>
    <div>For questions and comments, please <a href="mailto:hqi@berkeley.edu">contact us.</a>
    </div>
  </div>

  <div class="foot">
    © RSS 2025 Dexterous Manipulation Workshop
  </div>
</div>


</body>

</html>