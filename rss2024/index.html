<!DOCTYPE html>
<html lang="en" xmlns="">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <script type="text/javascript"
          src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>2nd Workshop on Dexterous Manipulation: Design, Perception and Control</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>
<div class="nav">
  <div class="nav-container">
    <a href="https://dex-manipulation.github.io/rss2024/index.html#intro">Introduction</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#speaker">Speakers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#call">Call for Papers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#schedule">Schedule</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#organizers">Organizers</a>
    <a href="https://dex-manipulation.github.io/rss2024/index.html#contact">Contact</a>
  </div>
</div>

<div class="title-container">
  <div style="text-align: center;">
    <h1>2nd Workshop on Dexterous Manipulation: <br> Design, Perception and Control</h1>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      Full-Day RSS 2024 Workshop, July 15, 2024
    </div>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      Live Stream on YouTube: <a href="https://www.youtube.com/watch?v=OdbTqSwsrkc">Morning Session</a>, <a
        href="https://youtube.com/live/8_r6rKoGaEw?feature=share">Afternoon Session</a>
    </div>
  </div>
</div>

<div class="container">
  <div class="section" id="intro">
    <h2>Introduction</h2>

    <p>
      This workshop aims to pioneer the exploration of learning-based strategies for achieving dexterous manipulation in
      robotics. Dexterous manipulation is among the most challenging problems in robotics, and this event seeks to
      provide insights and viewpoints for researchers and attendees on this subject. We will delve into current research
      and advancements in the hardware design of dexterous manipulators, generalizable manipulation learning approaches,
      and sensing technologies like tactile sensors.
    </p>
    <p>
      Following our <a href="https://learn-dex-hand.github.io/rss2023/" target="_blank"> first dexterous manipulation
      workshop in RSS 2023</a>. This workshop is Intended for a wide range of audiences, including researchers,
      engineers, and students with a strong foundation or interests in machine learning, computer vision, robotics, and
      related fields. The workshop promises a rich dialogue among leading experts from academic and industrial
      backgrounds. Accepted papers will have a chance to be presented in poster sessions and contributed talks. Through
      these events, participants will gain insights into the latest breakthroughs and collaborative opportunities,
      setting the stage for future innovations in robotic learning.

    </p>

  </div>


  <div class="section" id="speakers">
    <h2>Invited Speakers</h2>
    <div class="people">
      <a href="https://ait.ethz.ch/people/sammyc">
        <img src="https://ait.ethz.ch/assets/people/sammyc.jpg">
        <div>Sammy Christen</div>
        <div class="aff">ETH Zurich</div>
      </a>
      <a href="https://www.me.columbia.edu/faculty/matei-ciocarlie">
        <img
            src="https://roam.me.columbia.edu/sites/default/files/styles/cu_crop/public/content/people/Ciocarlie_headshot_midres.jpg?itok=NyhzjpyK">
        <div>Matei Ciocarlie</div>
        <div class="aff">Columbia University</div>
      </a>
      <a href="https://people.csail.mit.edu/kdoherty/">
        <img src="https://people.csail.mit.edu/kdoherty/assets/img/prof_pic.jpg">
        <div>Kevin Doherty</div>
        <div class="aff">Boston Dynamics</div>
      </a>
      <a href="https://tesshellebrekers.com/">
        <img src="https://tesshellebrekers.com/wp-content/uploads/2020/10/tess_edit-scaled.jpg">
        <div>Tess Hellebrekers</div>
        <div class="aff">Meta FAIR</div>
      </a>
      <a href="https://jhugestar.github.io/">
        <img src="https://jhugestar.github.io/img/han_dec_2017.jpg">
        <div>Hanbyul Joo</div>
        <div class="aff">Seoul National University</div>
      </a>
      <a href="https://www.tu.berlin/robotics">
        <img src="https://srl.ethz.ch/the-group/prof-robert-katzschmann.person_image.jpeg?persid=MjYyNzY1">
        <div>Robert Katzschmann</div>
        <div class="aff">ETH Zurich</div>
      </a>
      <a href="https://people.eecs.berkeley.edu/~malik/">
        <img src="https://people.eecs.berkeley.edu/~malik/malik-color1.jpg">
        <div>Jitendra Malik</div>
        <div class="aff">UC Berkeley</div>
      </a>
      <a href="https://hughw19.github.io/">
        <img src="https://hughw19.github.io/assets/photo1.png">
        <div>He Wang</div>
        <div class="aff">Peking University</div>
      </a>
    </div>
    <p> (<small><i>listed alphabetically</i></small>) </p>
  </div>

  <br>

  <div class="section" id="schedule">
    <h2>Workshop Schedule</h2>
    <ul></ul>
    <style type="text/css">
        .tg .tg-u4qn {
            background-color: #D9D9D9;
            text-align: left;
            vertical-align: bottom
        }
    </style>
    <table>
      <tr>
        <th>Time (GMT+1)</th>
        <th>Event</th>
      </tr>
      <tr>
        <td>08:45 - 08:50</td>
        <td>Introduction and Opening Remark</td>
      </tr>
      <tr>
        <td>08:50 - 09:15</td>
        <td>Invited Talk (Sammy Christen): <b>Physically Plausible and Natural Synthesis of Hand-Object Interactions</b>
        </td>
      </tr>
      <tr>
        <td>09:15 - 09:40</td>
        <td>Invited Talk (Hanbyul Joo): <b>Towards capturing everyday movements to scale up and enrich human motion
          data</b></td>
      </tr>
      <tr>
        <td>09:40 - 10:10</td>
        <td>Spotlight Talks I</td>
      </tr>
      <tr>
        <td>10:10 - 10:30</td>
        <td>Coffee Break + Poster Session + Real Robot Demo Session</td>
      </tr>
      <tr>
        <td>10:30 - 10:55</td>
        <td>Poster Session + Real Robot Demo Session</td>
      </tr>
      <tr>
        <td>10:55 - 11:20</td>
        <td>Invited Talk (He Wang): <b>Learning Generalizable Dexterous Grasping in Cluttered Environment via
          Sim2Real</b></td>
      </tr>
      <tr>
        <td>11:20 - 11:45</td>
        <td>Invited Talk (Robert Katzschmann): <b>Giving Robots a Hand</b></td>
      </tr>
      <tr>
        <td>11:45 - 12:10</td>
        <td>Invited Talk (Kevin Doherty): <b>KFC Fusion: Cooking up Kinematics, Forces, and Camera Data for Object State
          Estimation</b></td>
      </tr>
      <tr>
        <td>12:10 - 14:00</td>
        <td>Lunch Break</td>
      </tr>
      <tr>
        <td>14:00 - 14:25</td>
        <td>Invited Talk (Matei Ciocarlie): <b>Aiming for Dexterity: A New Hand Design with a New F/T Sensor</b></td>
      </tr>
      <tr>
        <td>14:25 - 14:50</td>
        <td>Invited Talk (Tess Hellebrekers): <b>Magnetic tactile sensing for robotic dexterity</b></td>
      </tr>
      <tr>
        <td>14:50 - 15:15</td>
        <td>Invited Talk (Jitendra Malik): <b>Learning Approaches to Dexterous Manipulation</b></td>
      </tr>
      <tr>
        <td>15:15 - 15:35</td>
        <td>Spotlight Talks II</td>
      </tr>
      <tr>
        <td>15:35 - 16:00</td>
        <td>Coffee Break + Poster Session + Real Robot Demo Session</td>
      </tr>
      <tr>
        <td>16:00 - 17:00</td>
        <td>Panel Discussion</td>
      </tr>
      <tr>
        <td>17:00 - 17:15</td>
        <td>Closing Remark & Award Ceremony</td>
      </tr>
    </table>
    <ul></ul>
  </div>
  <br>

  <div class="section" id="call">
    <h2>Call for Papers/Demos</h2>
    <p>
      In this workshop, our goal is to bring together researchers from various fields of robotics, such as control,
      optimization, learning, planning, sensing, hardware, etc., who work on dexterous manipulation.
    </p>

    <p>
      We are particularly excited to offer a platform for showcasing real-world robotic systems.
      <b style="color: darkblue">Even without a formal paper, we encourage submissions of videos demonstrating your
        robots in action. For those
        able to attend in person, there will be opportunities to showcase your robots live at the workshop!</b>
      We encourage
      researchers to submit work in the following areas (the list is not exhaustive):
    </p>

    <ul>
      <li><strong>Learning for Dexterous Manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>Can we expect a foundation policy for most daily dexterous manipulation tasks, or will-->
      <!--          each task require its own policy?-->
      <!--        </li>-->
      <!--        <li>How can learning-based policies handle dynamic tasks that require high-frequency control?-->
      <!--        </li>-->
      <!--        <li>-->
      <!--          How can we improve the generalization capability or the sample efficiency of policy learning in dexterous-->
      <!--          manipulation?-->
      <!--        </li>-->
      <!--      </ul>-->
      <li><strong>Planning and Optimization in Dexterous Manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>How to scale up planning/optimization methods to high DoF systems?-->
      <!--        </li>-->
      <!--        <li>-->
      <!--          How to leverage both planning/optimization methods and learning methods to get the best of both worlds?-->
      <!--        </li>-->
      <!--        <li>How can we make dexterous hands compliant?</li>-->
      <!--      </ul>-->
      <li><strong>Simulation for contact-rich manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>How to better model the contacts between the hand and object?-->
      <!--        </li>-->
      <!--        <li>How can we speed up simulation in the presence of frequent contacts?-->
      <!--        </li>-->
      <!--      </ul>-->
      <li><strong>Hardware design for Dexterous Manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>Is it possible to build a low-cost dexterous hand?-->
      <!--        </li>-->
      <!--        <li>What kind of hand design can make manipulation easier?-->
      <!--        </li>-->
      <!--        <li>-->
      <!--          What benefits do soft hands bring?-->
      <!--        </li>-->
      <!--        <li>-->
      <!--          Novel mechanisms that improve robotic hand performance.-->
      <!--        </li>-->
      <!--      </ul>-->
      <li><strong>Data collection in Dexterous Manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>How do we leverage human hand data in the wild to improve policy learning in dexterous manipulation?-->
      <!--        </li>-->
      <!--        <li>How to develop low-cost, easy-to-use teleoperation systems for large-scale demonstration data?-->
      <!--        </li>-->
      <!--        <li>How do we leverage planning methods to generate large-scale demonstration data in simulation?</li>-->
      <!--      </ul>-->
      <li><strong>Tactile Sensing for Manipulation</strong></li>
      <!--      <ul>-->
      <!--        <li>What's the role of tactile sensing for dexterous manipulation?-->
      <!--        </li>-->
      <!--        <li>-->
      <!--          How to use tactile data efficiently for control or policy learning?-->
      <!--        </li>-->
      <!--      </ul>-->

      <li>Any additional related topics not already covered in the above list ðŸ˜„</li>
    </ul>
    <h3>Submission Guidelines</h3>
    <p>
    <ul>
      <li><b style="color: darkblue">Submission Portal for Paper and Demo:</b> <a
          href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2024/Workshop/DM"
          target="_blank">OpenReview</a>
      </li>
      <li><b style="color: darkblue">Paper Submission Guideline:</b></li>
      <ul>
        <li><b>Paper Length:</b>
          Submissions could be <b>4-page</b> short papers, excluding references, acknowledgements, and appendices.
        </li>
        <li><b>Format:</b>
          <ul>
            <li>The format requirement follows the RSS 2024 main conference. Paper template: <a
                href="https://roboticsconference.org/docs/paper-template-latex.tar.gz" target="_blank">LaTeX</a> and <a
                href="https://roboticsconference.org/docs/paper-template-word.zip" target="_blank">Word</a></li>
            <li>Please include the references and appendix in the same PDF as the main paper and submit optional videos
              as zip file in supplementary.
            </li>
            <li>The maximum file size is 100MB. More information on
              format can be found <a
                  href="https://roboticsconference.org/information/authorinfo/#paper-and-demo-format"
                  target="_blank">here</a>
            </li>
          </ul>
        </li>
        <li><b>Dual Submission for Other Conference:</b>
          <ul>
            <li>Papers to be submitted or in preparation for submission to other major venues (including CoRL 2024) in
              the
              field are allowed.
            </li>
            <li>We also welcome published works as long as explicitly stated at the time of submission.</li>
          </ul>
        </li>
      </ul>

      <li><b style="color: darkblue">Demo Submission Guideline:</b></li>
      <ul>
        <li><b>Summary Document:</b>
          Please upload a summary document as a pdf. There is no page and format requirement for this pdf.
        </li>
        <li><b>Demo Video:</b>
          Please upload video demos as a single zip under the supplementary material field in OpenReview.
        </li>
      </ul>
      <li><b style="color: darkblue">Visibility:</b> Submissions and reviews will not be public. Only accepted papers
        will be made public.
      </li>
    </ul>

    <h3>Timeline</h3>
    <ul>
      <li>Submission Port Open: <b>Monday, April 15, 2024</b></li>
      <li>Submission Deadline:
        <del>Monday, June 10, 2024</del>
        &nbsp <b style="color: red;">Friday, June 14, 2024</b></li>
      <li>Notification: <b>Monday, July 1, 2024</b></li>
      <li>Workshop Date: <b>Monday, July 15, 2024</b></li>
    </ul>
  </div>

  <br>

  <div class="section" id="postersessions">
    <h2>Poster Sessions</h2>
    <!--    <p>Poster session assignments are posted below. The session will be held at .-->

    <table style="width: 100%">
      <thead>
      <tr>
        <th></th>
        <th>Spotlight Session I (09:40-10:10)</th>
        <th></th>
        <th>Spotlight Session II (15:15-15:35)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
        <td>Paper Name</td>
        <td>Authors</td>
        <td>Paper Name</td>
        <td>Authors</td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=KwwJuZIBXH'>3D Diffusion Policy: Generalizable Visuomotor Policy
          Learning via Simple 3D Representations</a>
        </td>
        <td>Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe Xu</td>
        <td><a href='https://openreview.net/forum?id=n2K5OWLf7C'>SpringGrasp: Synthesizing Compliant, Dexterous Grasps
          under Shape Uncertainty
        </a>
        </td>
        <td>Sirui Chen, Jeannette Bohg, Karen Liu</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=EiarCRjOm7'>DexCap: Scalable and Portable Mocap Data Collection
          System for Dexterous Manipulation</a>
        </td>
        <td>Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, Karen Liu</td>
        <td><a href='https://openreview.net/forum?id=EvapfdGjfY'>Tilde: Teleoperation for Dexterous In-Hand Manipulation
          Learning with a DeltaHand
        </a>
        </td>
        <td>Zilin Si, Kevin Lee Zhang, Zeynep Temel, Oliver Kroemer</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=d8qYLDH2vj'>ContactMPC: Towards Online Adaptive Control for
          Contact-Rich Dexterous Manipulation</a>
        </td>
        <td>Arjun Lakshmipathy, Nancy S. Pollard</td>
        <td><a href='https://openreview.net/forum?id=V6XHKFicwc'>DexGraspNet 2.0: Learning Generative Dexterous Grasping
          in Large-scale Synthetic Cluttered Scenes
        </a>
        </td>
        <td>Jialiang Zhang, Haoran Liu, Danshi Li, Xinqiang Yu, Haoran Geng, Yufei Ding, Jiayi Chen, He Wang</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=7sKUwZvTD6'>Tactile Exploration with Particle-Based Belief
          Entropy</a>
        </td>
        <td>Lara BrudermÃ¼ller, Julius Jankowski, Sylvain Calinon, Marc Toussaint, Nick Hawes</td>
        <td><a href='https://openreview.net/forum?id=3deVq5EoIZ'>Next-Gen Manipulation: An Active Surface-Based
          Underactuated Gripper with Visual Feedback
        </a>
        </td>
        <td>Karthik Swaminathan, Saad Hashmi, Alqama Shaikh, Vikas Phalle</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=pqgoIGvcgY'>RP1M: A Large-Scale Motion Dataset for Piano Playing
          with Bi-Manual Dexterous Robot Hands</a>
        </td>
        <td>Le Chen, Yi Zhao, Jan Schneider, Quankai Gao, Juho Kannala, Bernhard SchÃ¶lkopf, Joni Pajarinen, Dieter
          BÃ¼chler
        </td>
        <td><a href='https://openreview.net/forum?id=mZGi8ANPxz'>PianoMime: Learning a Generalist, Dexterous Piano
          Player from Internet Demonstrations
        </a>
        </td>
        <td>Cheng Qian, Julen Urain, Kevin Zakka, Jan Peters</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=eQomRzRZEP'>LEAP Hand V2: Dexterous, Low-cost Anthropomorphic
          Hybrid Rigid Soft Hand for Robot Learning</a>
        </td>
        <td>Kenneth Shaw, Deepak Pathak</td>
        <td><a href='https://openreview.net/forum?id=2uDOjKQG85'>Seahorse Tail-inspired Soft Pneumatic Actuator
          Utilizing Dual-mode Actuation
        </a>
        </td>
        <td>Dickson Chiu Yu Wong, Zijian Zhou, Garmisch Lai Yin Wong, Rob B.N. Scharff</td>
      </tr>
      </tbody>

      <tr>
        <td><a href='https://openreview.net/forum?id=iO0lDp0fSy'>OmniH2O: Universal and Dexterous Human-to-Humanoid
          Whole-Body Teleoperation and Learning</a>
        </td>
        <td>Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris M. Kitani, Changliu Liu,
          Guanya Shi
        </td>
        <td><a href='https://openreview.net/forum?id=edVs2ht61C'>A Dexterous Robotic Hand for In-Hand Manipulation of
          Long, Thin Objects
        </a>
        </td>
        <td>Abdullah Nazir</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=paxFR7SIHs'>ArtiGrasp: Physically Plausible Synthesis of Bi-Manual
          Dexterous Grasping and Articulation</a>
        </td>
        <td>Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo, Jie Song, Otmar Hilliges</td>
        <td><a href='https://openreview.net/forum?id=Jy61QPqTUu'>CyberDemo: Augmenting Simulated Human Demonstration for
          Real-World Dexterous Manipulation
        </a>
        </td>
        <td>Jun Wang, Yuzhe Qin, Kaiming Kuang, Yigit Korkmaz, Akhilan Gurumoorthy, Hao Su, Xiaolong Wang</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=YyBZdkD6ZQ'>A Compliant Gripper System for Delicate Object Grasping
          through Intrinsic Contact Sensing</a>
        </td>
        <td>Tao Yu, Jiepeng Wang, Shujie Tang, Xingyue Zhu, Genliang Chen</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=LnFqv4F8bl'>Sarcomere Dynamics - High Dexterity Robotic Hands</a>
        </td>
        <td>Harpal Mandaher</td>
      </tr>

    </table>
  </div>
  <br>

  <div class="section" id="organizers">
    <h2>Organizers</h2>
    <div class="people">
      <a href="https://taochenshh.github.io/">
        <img src="https://taochenshh.github.io/media/tao_profile.jpg">
        <div>Tao Chen</div>
        <div class="aff">MIT</div>
      </a>
      <a href="https://haozhi.io/">
        <img src="https://haozhi.io/profile.jpg">
        <div>Haozhi Qi</div>
        <div class="aff">UC Berkeley</div>
      </a>
      <a href="https://yzqin.github.io/">
        <img src="https://yzqin.github.io/file/qyz_circle.png">
        <div>Yuzhe Qin</div>
        <div class="aff">UC San Diego</div>
      </a>
      <a href="https://dingry.github.io/">
        <img src="https://dingry.github.io/content/images/avatar.png">
        <div>Runyu Ding</div>
        <div class="aff">University of Hong Kong</div>
      </a>
      <a href="https://contactrika.github.io/">
        <img src="https://contactrika.github.io/img/rika_photo_one.jpg">
        <div>Rika Antonova</div>
        <div class="aff">University of Cambridge</div>
      </a>
      <a href="https://asmorgan24.github.io/">
        <img src="images/Morgan.jpeg">
        <div>Andrew Morgan</div>
        <div class="aff">Boston Dynamics <br> AI Institute</div>
      </a>
      <a href="https://xiaolonw.github.io/">
        <img src="https://xiaolonw.github.io/static/profile.jpg">
        <div>Xiaolong Wang</div>
        <div class="aff">UC San Diego</div>
      </a>
      <a href="https://people.csail.mit.edu/pulkitag/">
        <img src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg">
        <div>Pulkit Agrawal</div>
        <div class="aff">MIT</div>
      </a>
    </div>
  </div>

  <div class="section" id="sponsor">
    <h2>Sponsor</h2>
    <div>
      <img src="images/dexmate.jpg" style="display: block; height: 120px " alt="DEXMATE">
    </div>
  </div>

  <div class="section" id="contact">
    <h2>Contact</h2>
    <div>For questions and comments, please <a href="mailto:taochen@mit.edu">contact us.</a>
    </div>
  </div>

  <div class="foot">
    Â© RSS 2024 Dexterous Manipulation Workshop
  </div>
</div>


</body>

</html>