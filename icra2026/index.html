<!DOCTYPE html>
<html lang="en" xmlns="">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <script type="text/javascript"
          src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Dexterity with Multifingered Hands: Hardware, Sensing, and Skills</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>
<div class="nav">
  <div class="nav-container">
    <a href="https://dex-manipulation.github.io/rss2025/index.html#intro">Introduction</a>
    <a href="https://dex-manipulation.github.io/rss2025/index.html#speaker">Speakers</a>
    <a href="https://dex-manipulation.github.io/rss2025/index.html#call">Call for Papers</a>
    <a href="https://dex-manipulation.github.io/rss2025/index.html#schedule">Schedule</a>
    <a href="https://dex-manipulation.github.io/rss2025/index.html#organizers">Organizers</a>
    <a href="https://dex-manipulation.github.io/rss2025/index.html#contact">Contact</a>
  </div>
</div>

<div class="title-container">
  <div style="text-align: center;">
    <h1>Dexterity with Multifingered Hands: <br> Hardware, Sensing, and Skills</h1>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      ICRA 2026 Workshop, June 1 or June 5, 2026
    </div>
<!--    <div class="subtitle" style="color: #ccc; margin: 20px">-->
<!--      <a href="https://www.youtube.com/watch?v=7a5HYjQ4wJo">[Recording]</a>-->
<!--    </div>-->
<!--    <div class="subtitle" style="color: #ccc; margin: 20px">-->
<!--      Location: OHE 122-->
<!--    </div>-->
<!--    <div class="subtitle" style="color: #ccc; margin: 20px">-->
<!--      Poster Stand Number: 35 - 60 (Epstein Plaza)-->
<!--    </div>-->
  </div>
</div>

<div class="container">
  <div class="section" id="intro">
    <h2>Introduction</h2>
    <p>
      Multi-fingered robotic hands promise versatile manipulation in human environments, from tool
      use and food preparation to cloth handling and knot tying. Recent advances in artificial
      intelligence, robust teleoperation, tactile sensing, and large-scale datasets are bringing us
      closer to overcoming long-standing challenges and realizing new levels of dexterity.
    </p>
    <p>
      Yet major gaps remain: multi-fingered hands often underperform compared to two-fingered
      grippers; datasets and benchmarks remain gripper-centric; and grippers already dominate
      industrial use. This raises a critical question: how can we overcome these limitations and unlock
      the full potential of dexterous hands?
    </p>
    <p>
      This workshop will bring together researchers from academia and industry to explore answers,
      with themes spanning manipulation bottlenecks, algorithmic advances, improved adaptability,
      and future directions. Through invited talks, spotlight presentations, posters, and a panel
      discussion, participants will exchange perspectives and build collaborations that drive progress
      toward practical dexterous manipulation.
    </p>
  </div>


  <div class="section" id="speakers">
    <h2>Invited Speakers</h2>

    <div class="people">

      <a href="https://srl.ethz.ch/the-group/prof-robert-katzschmann.html">
        <img src="../profiles/rk.jpg">
        <div>Robert Katzschmann</div>
        <div class="aff">ETH Zurich</div>
      </a>

      <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/SONG-Jie/jsongroas">
        <img src="../profiles/js.jpeg">
        <div>Jie Song</div>
        <div class="aff">HKUST (GZ)</div>
      </a>

      <a href="https://www.me.columbia.edu/faculty/matei-ciocarlie">
        <img src="../profiles/mc.jpg">
        <div>Matei Ciocarlie</div>
        <div class="aff">Columbia University</div>
      </a>

      <a href="https://lepora.com/">
        <img src="../profiles/nl.webp">
        <div>Nathan Lepora</div>
        <div class="aff">University of Bristol</div>
      </a>

      <a href="https://web.stanford.edu/~bohg/">
        <img src="../profiles/jb.png">
        <div>Jeannette Bohg</div>
        <div class="aff">Stanford</div>
      </a>

      <a href="https://people.csail.mit.edu/pulkitag/">
        <img src="../profiles/pa.jpg">
        <div>Pulkit Agrawal</div>
        <div class="aff">MIT</div>
      </a>

      <a href="https://contactrika.github.io/">
        <img src="../profiles/rika_antonova.jpg">
        <div>Rika Antonova</div>
        <div class="aff">University of Cambridge</div>
      </a>

      <a href="https://pearl-lab.com/people/georgia-chalvatzaki/">
        <img src="../profiles/georgia_chalvatzaki.jpg">
        <div>Georgia Chalvatzaki</div>
        <div class="aff">TU Darmstadt</div>
      </a>

      <a href="https://mariabauza.github.io/website/index.html">
        <img src="../profiles/maria_bauza_villalonga.jpg">
        <div>Maria Bauza Villalonga</div>
        <div class="aff">Google Deepmind</div>
      </a>

    </div>
    <p> (<small><i>listed alphabetically</i></small>) </p>
  </div>
  <br>

  <div class="section" id="schedule">
    <h2>Workshop Schedule (TBD)</h2>

<!--    <style type="text/css">-->
<!--        .tg .tg-u4qn {-->
<!--            background-color: #D9D9D9;-->
<!--            text-align: left;-->
<!--            vertical-align: bottom-->
<!--        }-->
<!--    </style>-->
<!--    <table>-->
<!--      <tr>-->
<!--        <th>Time (UTC -8)</th>-->
<!--        <th>Event</th>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>08:55 - 09:00</td>-->
<!--        <td>Introduction and Opening Remark</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>09:00 - 09:30</td>-->
<!--        <td>Invited Talk (Karen Liu)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>09:30 - 10:00</td>-->
<!--        <td>Invited Talk (Sudharshan Suresh)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>10:00 - 10:30</td>-->
<!--        <td>Invited Talk (Jessica Yin)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>10:30 - 11:00</td>-->
<!--        <td>Spotlight Session 1:-->
<!--          <ul>-->
<!--            <li> ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation </li>-->
<!--            <li> Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy </li>-->
<!--            <li> HuDOR: Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards </li>-->
<!--            <li> RUKA: Rethinking the Design of Humanoid Hands with Learning </li>-->
<!--            <li> DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation </li>-->
<!--            <li> Scaffolding Dexterous Manipulation with Vision-Language Models </li>-->
<!--          </ul>-->
<!--        </td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>11:00 - 11:30</td>-->
<!--        <td>Poster / Demo Session</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>11:30 - 12:00</td>-->
<!--        <td>Invited Talk (Ankur Handa)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>14:00 - 14:30</td>-->
<!--        <td>Invited Talk (Russ Tedrake)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>14:30 - 15:00</td>-->
<!--        <td>Invited Talk (Wanxin Jin)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>15:00 - 15:30</td>-->
<!--        <td>Invited Talk (Huazhe Xu)</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>15:30 - 16:00</td>-->
<!--        <td>Spotlight Session 2-->
<!--          <ul>-->
<!--            <li> DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies </li>-->
<!--            <li> Reinforcement Learning for Ambidextrous Bimanual Manipulation via Morphological Symmetry </li>-->
<!--            <li> Learning Particle-Based World Model from Human for Robot Dexterous Manipulation </li>-->
<!--            <li> DEXOS: Hand Exoskeleton System for Teaching Robot Dexterous Manipulation In-The-Wild </li>-->
<!--            <li> Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation </li>-->
<!--            <li> ImVR: Immersive VR Teleoperation System for General Purpose </li>-->
<!--            <li> Dexterous Contact-Rich Manipulation via the Contact Trust Region </li>-->
<!--          </ul>-->
<!--        </td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>16:00 - 16:30</td>-->
<!--        <td>Poster / Demo Session</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>16:30 - 17:25</td>-->
<!--        <td>Panel Discussion</td>-->
<!--      </tr>-->
<!--      <tr>-->
<!--        <td>17:25 - 17:30</td>-->
<!--        <td>Closing Remark & Award Ceremony</td>-->
<!--      </tr>-->
<!--    </table>-->
  </div>
  <br>

<!--  <div class="section" id="papers">-->
<!--    <h2>Accepted Papers</h2>-->
<!--    <ul>-->
<!--      <li><a href='https://openreview.net/forum?id=5udqMOeai1'>Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=beQYTjx17x'>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=c8wl1O12GH'>FLARE: Robot Learning with Implicit World Modeling</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=u3rYstjrpk'>Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=Lsej3WBwQj'>DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=ScRpDkgNJL'>DexNoMa: Learning Geometry-Aware Nonprehensile Dexterous Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=wmxeNgRUTh'>X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=lujxPiu99k'>ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=3zuBUoic2n'>Learning Particle-Based World Model from Human for Robot Dexterous Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=Sxwc7oo9tg'>Reinforcement Learning for Ambidextrous Bimanual Manipulation via Morphological Symmetry</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=S0FmCZ6by5'>DREAM: Differentiable Real-to-Sim-to-Real Engine for Learning Robotic Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=no55YX03LT'>Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=M2uezh5gZ2'>HuDOR: Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=Uf6SfYxjld'>RUKA: Rethinking the Design of Humanoid Hands with Learning</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=chOIumE4X5'>ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=XSdKoJKyH2'>Scaffolding Dexterous Manipulation with Vision Language Models</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=FVNOREWicy'>ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=97wHM52YU3'>DEXOP: Hand Exoskeleton System for Teaching Robot Dexterous Manipulation In-The-Wild</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=Y6ktqKRDeZ'>Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=7xCorSm0p7'>eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=C5GMHp14ib'>Touch begins where vision ends: Generalizable policies for contact-rich manipulation</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=JWF70OQxTS'>Learning Dexterous Deformable Object Manipulation Through Cross-Embodiment Dynamics Learning</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=26wkb05i8h'>ImVR: Immersive VR Teleoperation System for General Purpose</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=1FkgX44cJW'>Dexterous Contact-Rich Manipulation via the Contact Trust Region</a></li>-->
<!--      <li><a href='https://openreview.net/forum?id=QM6nVfejPd'>DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation</a></li>-->
<!--    </ul>-->
<!--  </div>-->

  <div class="section" id="call">
    <h2>Call for Papers/Demos (TBD)</h2>
<!--    <p>-->
<!--      In this workshop, our goal is to bring together researchers from various fields of robotics, such as control,-->
<!--      optimization, learning, planning, sensing, hardware, etc., who work on dexterous manipulation.-->
<!--    </p>-->

<!--    <p>-->
<!--  Each accepted short paper will be eligible for a poster presentation. Selected papers will also have the opportunity to give a short spotlight talk.-->
<!--  <b>Note: Both poster and spotlight presentations must be given in person.</b>-->
<!--    </p>-->

<!--    <p>-->
<!--      We are particularly excited to offer a platform for showcasing real-world robotic systems.-->
<!--      <b style="color: darkblue">Even without a formal paper, we encourage submissions of videos demonstrating your-->
<!--        robots in action. For those-->
<!--        able to attend in person, there will be opportunities to showcase your robots live at the workshop!</b>-->
<!--      We encourage-->
<!--      researchers to submit work in the following areas (the list is not exhaustive):-->
<!--    </p>-->

<!--    <ul>-->
<!--      <li><strong>Learning for Dexterous Manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>Can we expect a foundation policy for most daily dexterous manipulation tasks, or will-->
<!--                each task require its own policy?-->
<!--              </li>-->
<!--              <li>How can learning-based policies handle dynamic tasks that require high-frequency control?-->
<!--              </li>-->
<!--              <li>-->
<!--                How can we improve the generalization capability or the sample efficiency of policy learning in dexterous-->
<!--                manipulation?-->
<!--              </li>-->
<!--            </ul>-->
<!--      <li><strong>Planning and Optimization in Dexterous Manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>How to scale up planning/optimization methods to high DoF systems?-->
<!--              </li>-->
<!--              <li>-->
<!--                How to leverage both planning/optimization methods and learning methods to get the best of both worlds?-->
<!--              </li>-->
<!--              <li>How can we make dexterous hands compliant?</li>-->
<!--            </ul>-->
<!--      <li><strong>Simulation for contact-rich manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>How to better model the contacts between the hand and object?-->
<!--              </li>-->
<!--              <li>How can we speed up simulation in the presence of frequent contacts?-->
<!--              </li>-->
<!--            </ul>-->
<!--      <li><strong>Hardware design for Dexterous Manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>Is it possible to build a low-cost dexterous hand?-->
<!--              </li>-->
<!--              <li>What kind of hand design can make manipulation easier?-->
<!--              </li>-->
<!--              <li>-->
<!--                What benefits do soft hands bring?-->
<!--              </li>-->
<!--              <li>-->
<!--                Novel mechanisms that improve robotic hand performance.-->
<!--              </li>-->
<!--            </ul>-->
<!--      <li><strong>Data collection in Dexterous Manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>How do we leverage human hand data in the wild to improve policy learning in dexterous manipulation?-->
<!--              </li>-->
<!--              <li>How to develop low-cost, easy-to-use teleoperation systems for large-scale demonstration data?-->
<!--              </li>-->
<!--              <li>How do we leverage planning methods to generate large-scale demonstration data in simulation?</li>-->
<!--            </ul>-->
<!--      <li><strong>Tactile Sensing for Manipulation</strong></li>-->
<!--            <ul>-->
<!--              <li>What's the role of tactile sensing for dexterous manipulation?-->
<!--              </li>-->
<!--              <li>-->
<!--                How to use tactile data efficiently for control or policy learning?-->
<!--              </li>-->
<!--            </ul>-->

<!--      <li>Any additional related topics not already covered in the above list ðŸ˜„</li>-->
<!--    </ul>-->
<!--    <h3>Submission Guidelines</h3>-->
<!--    <p>-->
<!--    <ul>-->
<!--      <li><b style="color: darkblue">Submission Portal for Paper and Demo:</b> <a-->
<!--          href="https://openreview.net/group?id=roboticsfoundation.org/RSS/2025/Workshop/Dex"-->
<!--          target="_blank">OpenReview</a>-->
<!--      </li>-->
<!--      <li><b style="color: darkblue">Paper Submission Guideline:</b></li>-->
<!--      <ul>-->
<!--        <li><b>Paper Length:</b>-->
<!--          Submissions could be <b>4-page</b> short papers, excluding references, acknowledgements, and appendices.-->
<!--        </li>-->
<!--        <li><b>Format:</b>-->
<!--          <ul>-->
<!--            <li>The format requirement follows the RSS 2025 main conference. Paper template: <a-->
<!--                href="https://roboticsconference.org/docs/paper-template-latex.tar.gz" target="_blank">LaTeX</a> and <a-->
<!--                href="https://roboticsconference.org/docs/paper-template-word.zip" target="_blank">Word</a></li>-->
<!--            <li>Please include the references and appendix in the same PDF as the main paper and submit optional videos-->
<!--              as zip file in supplementary.-->
<!--            </li>-->
<!--            <li>The maximum file size is 100MB. More information on-->
<!--              format can be found <a-->
<!--                  href="https://roboticsconference.org/information/authorinfo/#paper-and-demo-format"-->
<!--                  target="_blank">here</a>.-->
<!--            </li>-->
<!--          </ul>-->
<!--        </li>-->
<!--        <li><b>Dual Submission for Other Conference:</b>-->
<!--          <ul>-->
<!--            <li>Papers to be submitted or in preparation for submission to other major venues (including CoRL 2024) in-->
<!--              the-->
<!--              field are allowed.-->
<!--            </li>-->
<!--            <li>We also welcome published works as long as explicitly stated at the time of submission.</li>-->
<!--          </ul>-->
<!--        </li>-->
<!--      </ul>-->

<!--      <li><b style="color: darkblue">Demo Submission Guideline:</b></li>-->
<!--      <ul>-->
<!--        <li><b>Summary Document:</b>-->
<!--          Please upload a summary document as a pdf. There is no page and format requirement for this pdf.-->
<!--        </li>-->
<!--        <li><b>Demo Video:</b>-->
<!--          Please upload video demos as a single zip under the supplementary material field in OpenReview.-->
<!--        </li>-->
<!--      </ul>-->
<!--      <li><b style="color: darkblue">Visibility:</b> Submissions and reviews will not be public. Only accepted papers-->
<!--        will be made public.-->
<!--      </li>-->
<!--    </ul>-->

<!--    <h3>Timeline</h3>-->
<!--    <ul>-->
<!--      <li>Submission Port Open: <b>May 7, 2025</b></li>-->
<!--      <li>Submission Deadline: <b style="color: red;"><del>May 29, 2025</del> June 1, 2025 (PT)</b></li>-->
<!--      <li>Notification: <b style="color: red;"><del>Jun 10, 2025</del> June 14, 2025 (PT)</b></li>-->
<!--      <li>Workshop Date: <b>June 25, 2025</b></li>-->
<!--    </ul>-->

  </div>

  <div class="section" id="organizers">
    <h2>Organizers</h2>
    <div class="people">
      <a href="https://haozhi.io/">
        <img src="../profiles/haozhi_qi.jpg">
        <div>Haozhi Qi</div>
        <div class="aff">Amazon FAR & University of Chicago</div>
      </a>
      <a href="https://kennyshaw.net/">
        <img src="../profiles/kenny_shaw.jfif">
        <div>Kenneth Shaw</div>
        <div class="aff">Carnegie Mellon University</div>
      </a>
      <a href="https://www.linkedin.com/in/shuang-li-098a35244/">
        <img src="../profiles/shuang_li.jpeg">
        <div>Shuang Li</div>
        <div class="aff">Agile Robots SE</div>
      </a>
      <a href="https://chengpan-web.github.io/">
        <img src="../profiles/cheng_pan.jpg">
        <div>Cheng Pan</div>
        <div class="aff">EPFL</div>
      </a>
      <a href="https://leizhang-public.github.io/">
        <img src="../profiles/lei_zhang.jpeg">
        <div>Lei Zhang</div>
        <div class="aff">University of Hamburg & Agile Robots SE</div>
      </a>
      <a href="https://www.mccormick.northwestern.edu/research-faculty/directory/profiles/lynch-kevin.html">
        <img src="../profiles/kevin_lynch.jpg">
        <div>Kevin Lynch</div>
        <div class="aff">Northwestern University</div>
      </a>
    </div>
  </div>

<!--  <div class="section" id="sponsor">-->
<!--    <h2>Sponsor</h2>-->
<!--    <div>-->
<!--      <img src="images/dexmate.jpg" style="display: block; height: 120px " alt="DEXMATE">-->
<!--    </div>-->
<!--  </div>-->

  <div class="section" id="contact">
    <h2>Contact</h2>
    <div>For questions and comments, please <a href="mailto:haozhi@uchicago.edu">contact us.</a>
    </div>
  </div>

  <div class="foot">
    Â© ICRA 2026 Dexterous Manipulation Workshop
  </div>
</div>


</body>

</html>