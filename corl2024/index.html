<!DOCTYPE html>
<html lang="en" xmlns="">

<head>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <script type="text/javascript"
          src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <title>Learning Robot Fine and Dexterous Manipulation: Perception and Control</title>
  <link rel="stylesheet" href="css/style.css">
</head>

<body>
<div class="nav">
  <div class="nav-container">
    <a href="https://dex-manipulation.github.io/corl2024/index.html#intro">Introduction</a>
    <a href="https://dex-manipulation.github.io/corl2024/index.html#speaker">Speakers</a>
    <a href="https://dex-manipulation.github.io/corl2024/index.html#call">Call for Papers</a>
    <a href="https://dex-manipulation.github.io/corl2024/index.html#schedule">Schedule</a>
    <a href="https://dex-manipulation.github.io/corl2024/index.html#organizers">Organizers</a>
    <a href="https://dex-manipulation.github.io/corl2024/index.html#contact">Contact</a>
  </div>
</div>

<div class="title-container">
  <div style="text-align: center;">
    <h1>Learning Robot Fine and Dexterous Manipulation: <br> Perception and Control </h1>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      CoRL 2024 Workshop, November 9, 2024
    </div>
    <div class="subtitle" style="color: #ccc; margin: 20px">
      <a href="">YouTube Live (link will be available on November 9)</a>
    </div>
  </div>
</div>

<div class="container">
  <div class="section" id="intro">
    <h2>Introduction</h2>

    <p> Fine manipulation involves making precise movements with robotic hands and fingers to handle small objects or perform intricate tasks, such as threading a needle. Dexterous manipulation goes further, requiring highly skilled, accurate, and versatile manipulation of objects through complex interactions among multiple fingers and joints. The ability to achieve fine and dexterous manipulation with high speed, accuracy, and dexterity is becoming increasingly important in robotics research. However, it also poses many challenges, such as frequent making and breaking of contact, real-time feedback control with high-dimensional observations, high-dimensional control spaces, and objects being in unstable configurations. Traditional methods rely on precise robot and environment models but often struggle with real-world uncertainties and lack generalizability. Despite decades of research, most demonstrations of dexterous manipulation still rely heavily on teleoperation. Achieving robust and generalizable dexterous manipulation requires advancements in perception integration, data collection, and control. Advances in robot learning, including machine learning and transfer learning, offer promising pathways to enhance robotic performance in fine and dexterous manipulation tasks. This event seeks to convene researchers from diverse disciplines to share insights on pushing this critical boundary. </p>

    <p> This workshop aims to bring together junior and senior researchers to discuss the latest advancements, challenges, and future directions in learning-based approaches for robot fine manipulation skills, one of the most challenging areas in robotics. We will delve into the current state-of-the-art across relevant areas, including the hardware and mechanical design of dexterous manipulators, generalizable skill learning techniques, and sensing modalities such as tactile sensors and vision systems. Researchers will have opportunities to present posters, give contributed talks, and engage in thought-provoking discussions.
    </p>

    <p>
      We will explore the following focused research questions:
    </p>

    <b> Visual Perception </b>
    <ul>
      <li> In the context of dexterous manipulation, how can we overcome challenges related to occlusions between objects and robot hands?
      <li> What are the strategies for generalizing manipulation policies to outdoor or unstructured environments, considering variables such as lighting changes and the need to process extensive information?
    </ul>

    <b> Tactile Perception </b>
    <ul>
      <li> How can tactile feedback be utilized in dexterous manipulation?
      <li> What types of tactile sensors and data are most beneficial for these tasks, and how can they complement or substitute visual feedback?
    </ul>

    <b> Robot Skill Learning </b>
    <ul>
<li> Foundation vs. Specialized Models: Is the future of dexterous manipulation in developing a foundation model for general tasks, or will specialized models for specific tasks prevail?
<li> Dynamic Tasks: How can learning-based approaches effectively manage dynamic manipulation that necessitates precise control and accurate dynamics modeling?
<li> Generalization of Human Hand Data: Can we standardize the collection of human hand data for dexterous manipulation using expert-grade equipment, and what strategies can bridge the data gap between human and robotic manipulation capabilities?
<li> Enhancing Data Collection Methods: What improvements can be made to teleoperation and other data collection methods to enable large-scale acquisition of manipulation data?
<li> Reducing Reward Engineering for Reinforcement Learning: Tuning reward functions is non-trivial for new tasks. How can we reduce the engineering effort in reward shaping? How can we scale up reinforcement learning pipelines to solve hundreds of different tasks?
</ul>

    <b> Learning Fine and Dexterous Manipulation Skills </b>
      <ul>
        <li>What are the main challenges in learning fine and dexterous manipulation skills compared to other robot skills?
        <li>Is end-to-end learning effective for acquiring these skills?
        <li>What challenges arise in coordinating surplus degrees of freedom during in-hand manipulation, and what are the requirements for learning techniques to address these challenges effectively?
        <li>How can various applications, such as prosthetics, benefit from emerging algorithms?
        <li>What challenges and prerequisites do touch processing and tactile sensing entail?
        <li>How can we establish robust benchmarks and frameworks for systematically evaluating and comparing the effectiveness of various learning techniques?
      </ul>
    <p>The expected attendees for these discussions and talks are researchers from academia and industry specializing in robotics and machine learning, particularly those interested in applying learning approaches to achieve fine and dexterous manipulation in robots.</p>

  </div>


  <div class="section" id="speakers">
    <h2>Invited Speakers</h2>
    <div class="people">
      <a href="https://people.csail.mit.edu/pulkitag/">
        <img src="https://people.csail.mit.edu/pulkitag/images/pulkit.jpg">
        <div>Pulkit Agrawal</div>
        <div class="aff">MIT</div>
      </a>
      <a href="https://aidx-lab.org/">
        <img src="https://aidx-lab.org/assets/imgs/group/baeuml-berthold.jpeg">
        <div>Berthold BÃ¤uml</div>
        <div class="aff">TUM</div>
      </a>

      <a href="https://www.dryaseminbekiroglu.com/">
        <img src="https://images.squarespace-cdn.com/content/v1/6601cd815cb5f946d6b37397/237da078-78a6-4bd1-ad08-926de9349583/YBekiroglu2022.jpg?format=750w">
        <div>Yasemin BekiroÄŸlu</div>
        <div class="aff">Chalmers University of Technology</div>
      </a>

      <a href="https://www.tu.berlin/en/robotics/about-rbo/prof-dr-oliver-brock">
        <img src="https://soma-summerschool.dlr.de/wp-content/uploads/2016/10/Oliver_Brock.jpg">
        <div>Oliver Brock</div>
        <div class="aff">TU Berlin</div>
      </a>

      <a href="https://www.robertocalandra.com/about/">
        <img src="https://pbs.twimg.com/profile_images/1641101263164022786/Sr57Toqs_400x400.jpg">
        <div>Roberto Calandra</div>
        <div class="aff">TU Dresden</div>
      </a>

      <a href="https://renaud-detry.net/">
        <img src="https://renaud-detry.net/assets/img/detry-480.webp">
        <div>Renaud Detry</div>
        <div class="aff">KU Leuven</div>
      </a>

      <a href="http://people.isir.upmc.fr/doncieux">
        <img src="https://www.isir.upmc.fr/wp-content/uploads/2021/02/photo_SD3.jpg">
        <div>StÃ©phane Doncieux</div>
        <div class="aff">ISIR</div>
      </a>

      <a href="https://animesh.garg.tech/">
        <img src="https://animesh.garg.tech/assets/img/animesh-sept19-sq.jpg">
        <div>Animesh Garg</div>
        <div class="aff">Georgia Tech</div>
      </a>

      <a href="https://faculty.utah.edu/u6002596-Tucker_Hermans/hm/index.hml">
        <img src="https://faculty.utah.edu/bytes/image.hml?id=u6002596&fullsize=true">
        <div>Tucker Hermans</div>
        <div class="aff">University of Utah</div>
      </a>

      <a href="https://jpsilverio.github.io/">
        <img src="https://jpsilverio.github.io/images/JoaoSilverio.jpg">
        <div>JoÃ£o SilvÃ©rio</div>
        <div class="aff"> German Aerospace Center (DLR)</div>
      </a>

      <a href="https://xiaolonw.github.io/">
        <img src="https://xiaolonw.github.io/static/profile.jpg">
        <div>Xiaolong Wang</div>
        <div class="aff">UC San Diego</div>
      </a>

      <a href="https://ericyi.github.io/">
        <img src="https://ericyi.github.io/Li_Yi_files/ericyi.jpg">
        <div>Li Yi</div>
        <div class="aff">THU</div>
      </a>

    </div>
    <p> (<small><i>listed alphabetically</i></small>) </p>
  </div>

  <br>

  <div class="section" id="schedule">
    <h2>Workshop Schedule</h2>
    <ul></ul>
    <style type="text/css">
        .tg .tg-u4qn {
            background-color: #D9D9D9;
            text-align: left;
            vertical-align: bottom
        }
    </style>

    <table>
      <tr>
        <th>Time (GMT+1, Germany Time Zone)</th>
        <th>Event</th>
      </tr>
      <tr>
        <td>08:30 - 08:35</td>
        <td>Introduction and Opening Remark</td>
      </tr>
      <tr>
        <td>08:35 - 08:55</td>
        <td>Invited Talk <b>(Oliver Brock)</b>: <br> Dexterous Manipulation: Why Learn It if You Can Just Do It?</td>
      </tr>
      <tr>
        <td>08:55 - 09:15</td>
        <td>Invited Talk <b>(Stephane Doncieux)</b>: <br> Learning to grasp: from dataset generation to application on real robots.</td>
      </tr>
      <tr>
        <td>09:15 - 09:35</td>
        <td>Invited Talk <b>(Yasemin Bekiroglu)</b>: <br>  Data-efficient learning from vision and touch for manipulation tasks.</td>
      </tr>
      <tr>
        <td>09:35 - 10:00</td>
        <td>Spotlight Presentations</td>
      </tr>
      <tr>
        <td>10:00 - 10:50</td>
        <td>Coffee Break and Poster Session</td>
      </tr>
      <tr>
        <td>10:50 - 11:10</td>
        <td>Invited Talk <b>(Animesh Garg)</b>: <br> Facets of Dexterity: Simulation, RL, and Imitation.</td>
      </tr>
      <tr>
        <td>11:10 - 11:30</td>
        <td>Invited Talk <b>(Pulkit Agrawal)</b>: <br> TBD</td>
      </tr>
      <tr>
        <td>11:30 - 11:50</td>
        <td>Invited Talk <b>(Berthold BÃ¤uml)</b>: <br> Autonomously Learning AI: Towards Human-Level Manipulation with Dextrous Hands.</td>
      </tr>
      <tr>
        <td>11:50 - 13:45</td>
        <td>Lunch Break</td>
      </tr>
      <tr>
        <td>13:50 - 14:10</td>
        <td>Invited Talk <b>(Tucker Hermans)</b>: <br> What's the right recipe for learning multi-fingered manipulation?</td>
      </tr>
      <tr>
        <td>14:10 - 14:30</td>
        <td>Invited Talk <b>(Roberto Calandra)</b>: <br> The Importance of Touch Sensing for Dexterous Manipulation.</td>
      </tr>
      <tr>
        <td>14:30 - 14:50</td>
        <td>Invited Talk <b>(Xiaolong Wang)</b>: <br> TBD</td>
      </tr>
      <tr>
        <td>14:50 - 15:15</td>
        <td>Spotlight Presentations</td>
      </tr>
      <tr>
        <td>15:15 - 16:05</td>
        <td>Coffee Break and Poster Session</td>
      </tr>
      <tr>
        <td>16:05 - 16:25</td>
        <td>Invited Talk <b>(Li Yi)</b>: <br> Learning Dexterous Manipulation from Human-Object Interaction.</td>
      </tr>
      <tr>
        <td>16:25 - 16:45</td>
        <td>Invited Talk <b>(Renaud Detry)</b>: <br>Assistive Manipulation with a Blend of Arm Trajectory Prediction and Scene Affordance Parsing.</td>
      </tr>
      <tr>
        <td>16:45 - 17:05</td>
        <td>Invited Talk <b>(JoÃ£o SilvÃ©rio)</b>: <br>Adaptive Learning and Assistance for Real-World Robotic Manipulation.</td>
      </tr>
      <tr>
        <td>17:05 - 17:50</td>
        <td>Panel Discussion</td>
      </tr>
      <tr>
        <td>17:50 - 18:00</td>
        <td>Closing Remark and Award</td>
      </tr>
    </table>
    <ul></ul>
  </div>
  <br>

  <div class="section" id="spotlight">
    <h2>Poster Sessions</h2>

    <h3>Spotlight Session I (09:35-10:00)</h3>
    <table style="width: 100%">
      <tbody>
      <tr>
        <td>Paper Name</td>
        <td>Authors</td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=r30rWbaHNq'>Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards</a>
        </td>
        <td>Irmak Guzey, Yinlong Dai, Georgy Savva, Raunaq Bhirangi, Lerrel Pinto</td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=xq1y3tkgAM'>
          Representing Positional Information in Generative World Models for Object Manipulation
        </a></td>
        <td>
          Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Sai Rajeswar
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=B7YeP2h08V'>
          Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile Skins
        </a></td>
        <td>
          Venkatesh Pattabiraman, Yifeng Cao, Siddhant Haldar, Lerrel Pinto, Raunaq Bhirangi
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=nTAT5y78Kk'>
          AnySkin: Plug-and-play Skin Sensing for Robotic Touch
        </a></td>
        <td>
          Raunaq Bhirangi, Venkatesh Pattabiraman, Mehmet Enes Erciyes, Yifeng Cao, Tess Hellebrekers, Lerrel Pinto
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=eOtDTS4iMQ'>
          EgoMimic: Scaling Imitation Learning via Egocentric Video
        </a></td>
        <td>
          Simar Kareer, Dhruv Patel, Ryan Punamiya, Pranay Mathur, Shuo Cheng, Chen Wang, Judy Hoffman, Danfei Xu
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=ZQJvH66GJY'>
          Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition
        </a></td>
        <td>
          Shengcheng Luo, Quanquan Peng, Jun Lv, Kaiwen Hong, Katherine Rose Driggs-Campbell, Cewu Lu, Yong-Lu Li
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=oDKc2H00ok'>
          The Role of Action Abstractions in Robot Manipulation Learning and Sim-to-Real Transfer
        </a></td>
        <td>
          Elie Aljalbout, Felix Frank, Maximilian Karl, Patrick van der Smagt
        </td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=xdLpHUvdoq'>
          DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control
        </a></td>
        <td>
          Zichen Jeff Cui, Hengkai Pan, Aadhithya Iyer, Siddhant Haldar, Lerrel Pinto
        </td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=hZ0DAHAsSX'>
          Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation
        </a></td>
        <td>
          Zhaoliang Wan, Zida Zhou, YANGZEHUI, Hao Ding, Senlin Yi, Zetong Bi, Hui Cheng
        </td>
      </tr>

            <tr>
        <td><a href='https://openreview.net/forum?id=ineQhdiElU'>
          DexiTac: Soft Dexterous Tactile Gripping
        </a></td>
        <td>
          Chenghua lu, Kailuan Tang, Max Yang, Haoran Li, Tianqi Yue, Nathan F. Lepora
        </td>
      </tr>

                  <tr>
        <td><a href='https://openreview.net/forum?id=1u8E6D0wCo'>
          Decomposing the Configuration of an Articulated Object via Graph Neural Network
        </a></td>
        <td>
          Seunghyeon Lim, Kisung Shin, Nakul Gopalan, Jun Ki Lee, Byoung-Tak Zhang
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=VjJhGuxUAw'>
          Learning to Accurately Throw Paper Planes
        </a></td>
        <td>
          Marcus Kornmann, Qimeng He, Alap Kshirsagar, Kai Ploeger, Jan Peters
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=wibyjvWmad'>
          Diff-HySAC: Diffusion-Based Hybrid Soft Actor-Critic for 6D Non-Prehensile Manipulation
        </a></td>
        <td>
          Huy Le, Miroslav Gabriel, Tai Hoang, Gerhard Neumann, Vien Anh Ngo
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=Agtbip6BWr'>
          MILES: Making Imitation Learning Easy with Self-Supervision
        </a></td>
        <td>
          Georgios Papagiannis, Edward Johns
        </td>
      </tr>

            <tr>
        <td><a href='https://openreview.net/forum?id=693ZYvzWwD'>
          TacEx: GelSight Tactile Simulation in Isaac Sim â€“ Combining Soft-Body and Visuotactile Simulators
        </a></td>
        <td>
          Duc Huy Nguyen, Guillaume Duret, Tim Schneider, Alap Kshirsagar, Boris Belousov, Jan Peters
        </td>
      </tr>


      <tr>
        <td><a href='https://openreview.net/forum?id=YE38dy5L62'>
          DROP: Dexterous Reorientation via Online Planning

        </a></td>
        <td>
          Albert H. Li, Preston Culbertson, Vince Kurtz, Aaron Ames
        </td>
      </tr>


        <tr>
        <td><a href='https://openreview.net/forum?id=x03eZePXzP'>
          ATK: Automatic Task-driven Keypoint selection for Policy Transfer from Simulation to Real World
        </a></td>
        <td>
          Yunchu Zhang, Zhengyu Zhang, Liyiming Ke, Siddhartha Srinivasa, Abhishek Gupta
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=tVqfWSfTXY'>
          Bimanual Dexterity for Complex Tasks
        </a></td>
        <td>
          Kenneth Shaw, Yulong Li, Jiahui Yang, Mohan Kumar Srirama, Muxin Liu, Haoyu Xiong, Russell Mendonca, Deepak Pathak
        </td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=saAyrKfQOL'>
          Object-Centric Dexterous Manipulation from Human Motion Data
        </a></td>
        <td>
          Yuanpei Chen, Chen Wang, Yaodong Yang, Karen Liu
        </td>
      </tr>

    </table>

    <br>
    <h3>Spotlight Session II (14:50-15:15)</h3>
    <table style="width: 100%">
      <tbody>
      <tr>
        <td>Paper Name</td>
        <td>Authors</td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=sLN9Q8anpm'>BAKU: An Efficient Transformer for Multi-Task Policy Learning</a>
        </td>
        <td>Siddhant Haldar, Zhuoran Peng, Lerrel Pinto</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=E7pzkbjdRl'>Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments</a>
        </td>
        <td>Haritheja Etukuru, Norihito Naka, Zijin Hu, Seungjae Lee, Chris Paxton, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=zc9hL6mlZJ'>ScissorBot: Learning Generalizable Scissor Skill for Paper Cutting via Simulation, Imitation, and Sim2Real</a>
        </td>
        <td>Jiangran Lyu, Yuxing Chen, Tao Du, Feng Zhu, Huiquan Liu, Yizhou Wang, He Wang</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=Fz5yxQopXa'>Local Policies Enable Zero-shot Long-horizon Manipulation</a>
        </td>
        <td>Murtaza Dalal, Min Liu, Walter Talbott, Chen Chen, Deepak Pathak, Jian Zhang, Russ Salakhutdinov</td>
      </tr>
      <tr>
        <td><a href='https://openreview.net/forum?id=Xx9PTRKnzH'>OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation</a>
        </td>
        <td>Aadhithya Iyer, Zhuoran Peng, Yinlong Dai, Irmak Guzey, Siddhant Haldar, Soumith Chintala, Lerrel Pinto</td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=0rsnbgcHUU'>
          D(R,O) Grasp: A Unified Representation of Robot and Object Interaction for Cross-Embodiment Dexterous Grasping
        </a></td>
        <td>
          Zhenyu Wei, Zhixuan Xu, Jingxiang Guo, Yiwen Hou, Chongkai Gao, Cai Zhehao, Jiayu Luo, Lin Shao
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=yzAd3vcAZR'>
          From Imitation to Refinement â€“ Residual RL for Precise Visual Assembly
        </a></td>
        <td>
          Lars Lien Ankile, Anthony Simeonov, Idan Shenfeld, Marcel Torne Villasevil, Pulkit Agrawal
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=LArywC3G3Z'>
          3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing
        </a></td>
        <td>
          Binghao Huang, Yixuan Wang, Xinyi Yang, Yiyue Luo, Yunzhu Li
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=SGXDTQ42q1'>
          Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control
        </a></td>
        <td>
          Tan-Dzung Do, Gireesh Nandiraju, Jilong Wang, He Wang
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=GHqUhqMahn'>
          Catch It! Learning to Catch in Flight with Mobile Dexterous Hands
        </a></td>
        <td>
          Yuanhang Zhang, Tianhai Liang, Zhenyang Chen, Yanjie Ze, Huazhe Xu
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=Bt5upUsbvu'>
          AnyRotate: Gravity-Invariant In-Hand Object Rotation with Sim-to-Real Touch
        </a></td>
        <td>
          Max Yang, chenghua lu, Alex Church, Yijiong Lin, Christopher J. Ford, Haoran Li, Efi Psomopoulou, David A.W. Barton, Nathan F. Lepora
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=1UE7mk2uiB'>
          ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data
        </a></td>
        <td>
          Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
        </td>
      </tr>

        <tr>
        <td><a href='https://openreview.net/forum?id=YPR0X7dCXn'>
          Diffusion Policy Policy Optimization
        </a></td>
        <td>
          Allen Z. Ren, Justin Lidard, Lars Lien Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, Max Simchowitz
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=q132HOiizE'>
          Probabilistic Contact Mode Planning for Multi-Finger Manipulation using Diffusion Models
        </a></td>
        <td>
          Thomas Power, Abhinav Kumar, Fan Yang, Sergio Francisco Aguilera Marinovic, Soshi Iba, Rana Soltani Zarrin, Dmitry Berenson
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=BMEEPCIWTf'>
          GeoMatch++: Morphology Conditioned Geometry Matching for Multi-Embodiment Grasping
        </a></td>
        <td>
          Yunze Wei, Maria Attarian, Igor Gilitschenski
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=KgUgavAl6Y'>
          DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning
        </a></td>
        <td>
          Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, Yuke Zhu
        </td>
      </tr>

      <tr>
        <td><a href='https://openreview.net/forum?id=QADznDlGM4'>
          DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning
        </a></td>
        <td>
          Zhen Zhang, Xiangyu Chu, TANG Yunxi, K. W. Samuel Au
        </td>
      </tr>

            <tr>
        <td><a href='https://openreview.net/forum?id=93UzkM03Vi'>
          Analysing the Interplay of Vision and Touch for Dexterous Insertion Tasks
        </a></td>
        <td>
          Janis Lenz, Theo Gruner, Daniel Palenicek, Tim Schneider, Inga Pfenning, Jan Peters
        </td>
      </tr>

    </table>

</div>


  <div class="section" id="call">
    <h2>Call for Papers/Demos</h2>

    <p>
      In this workshop, our goal is to bring together researchers from various fields of robotics,
      such as control, optimization, learning, planning, sensing, hardware, etc., who work on dexterous manipulation.
      We encourage researchers to submit work in the following areas (the list is not exhaustive):
    </p>

    <!--    <p>-->
    <!--      We are particularly excited to offer a platform for showcasing real-world robotic systems.-->
    <!--      <b style="color: darkblue">Even without a formal paper, we encourage submissions of videos demonstrating your-->
    <!--        robots in action. For those-->
    <!--        able to attend in person, there will be opportunities to showcase your robots live at the workshop!</b>-->
    <!--      We encourage-->
    <!--      researchers to submit work in the following areas (the list is not exhaustive):-->
    <!--    </p>-->

    <ul>
      <li>In-hand manipulation, dexterous manipulation, fine manipulation
      <li>Learning robot fine-manipulation skills
      <li>Reinforcement learning in robotic fine-manipulation
      <li>Transfer learning in robotic fine-manipulation
      <li>Imitation learning/Learning from demonstration of fine-manipulation skills
      <li>Data collection for fine and/or dexterous manipulation
      <li>Simulation for fine and/or dexterous manipulation (sim2real approaches)
      <li>Investigating planning strategies and algorithms for dexterous manipulation tasks
      <li>Integrating tactile sensing for enhanced manipulation and multi-modal learning
      <li>Welcoming submissions on any additional topics related to learning-based dexterous manipulation not covered
        above ðŸ˜„
    </ul>
    <h3>Submission Guidelines</h3>
        <ul>
          <li><b style="color: darkblue">Submission Portal for Paper and Demo:</b> <a
              href="https://openreview.net/group?id=robot-learning.org/CoRL/2024/Workshop/LFDM"
              target="_blank">OpenReview</a>
          </li>
          <li><b style="color: darkblue">Paper Submission Guideline:</b></li>
          <ul>
            <li><b>Paper Length:</b>
              Submissions should be <b>4-page</b> maximum short papers, excluding references, acknowledgements, and appendices.
            </li>
            <li><b>Format:</b>
              <ul>
                <li>The format requirement follows the CoRL 2024 main conference. Paper template: <a
                        href="https://drive.google.com/file/d/1mPoPyHJWAfLlgAIEhWKov7Crywsz9c1M/view?usp=sharing" target="_blank"> Link</a>. Please make sure your submission is anonymous.
                <li>Please include the references and appendix in the same PDF as the main paper and submit optional videos
                  as zip file in supplementary.
                </li>
                <li>The maximum file size is 100MB. More information on
                  format can be found <a
                      href="https://www.corl.org/contributions/instruction-for-authors"
                      target="_blank">here</a>.
                </li>
              </ul>
            </li>
            <li>
            <b> Review Process:</b>
              <ul>
                <li>All submissions will undergo a peer-review process and will be evaluated based on their relevance and contribution to the workshopâ€™s topics.
                <li>Accepted submissions will be featured at the workshop through Poster Spotlight Talks (1.5-minute presentations) and Poster Sessions.
                <li>They will also be published on the workshopâ€™s websites, along with an optional 1.5-minute video summary.
              </ul>
            </li>
            <li><b>Dual Submission for Other Conference:</b>
              <ul>
                <li>Papers to be submitted or in preparation for submission to other major venues in the
                  field are allowed.
                </li>
                <li>We also welcome published works as long as explicitly stated at the time of submission.</li>
              </ul>
            </li>
          </ul>

          <li><b style="color: darkblue">Visibility:</b> Submissions and reviews will not be public. Only accepted papers
            will be made public.
          </li>
        </ul>

        <h3>Timeline</h3>
        <ul>
          <li>Submission Deadline: <b> October 6, 2024 </b></li>
          <li>Notification: <b>October 14, 2024</b></li>
          <li>Workshop Date: <b>November 9, 2024</b></li>
        </ul>
  </div>

  <br>
  <!--  <div class="section" id="postersessions">-->
  <!--    <h2>Poster Sessions (TBD after paper acceptance notification)</h2>-->
  <!--  </div>-->

  <div class="section" id="organizers">
    <h2>Organizers</h2>

    Remark: This workshop is the result of combining two similar workshop proposals that were both accepted to be held at CoRL 2024.

    <br> "Learning Robot Fine and Dexterous Manipulation Skills" by

    <div class="people">
      <a href="https://people.epfl.ch/soheil.gholami?lang=en">
        <img src="https://people.epfl.ch/private/common/photos/links/351072.jpg?ts=1725910374">
        <div>Soheil Gholami</div>
        <div class="aff">EPFL</div>
      </a>

      <a href="https://people.epfl.ch/xiao.gao">
        <img src="https://people.epfl.ch/private/common/photos/links/348857.jpg?ts=1725910374">
        <div>Xiao Gao</div>
        <div class="aff">EPFL</div>
      </a>

      <a href="https://newmanlab.mit.edu/lab_members/kunpeng-yao/">
        <img src="https://newmanlab.mit.edu/wp-content/uploads/2024/05/Kunpeng-Yao-portrait-2024-480x480.jpg">
        <div>Kunpeng Yao</div>
        <div class="aff">MIT</div>
      </a>

      <a href="https://people.epfl.ch/aude.billard">
        <img src="https://people.epfl.ch/private/common/photos/links/115671.jpg?ts=1725910376">
        <div>Aude Billard</div>
        <div class="aff">EPFL</div>
      </a>
    </div>

    "Learning Dexterous Manipulation: Design, Perception, Data Collection, and Control" by <br>

    <div class="people">
    <a href="https://haozhi.io/">
      <img src="https://haozhi.io/profile.jpg">
      <div>Haozhi Qi</div>
      <div class="aff">UC Berkeley</div>
    </a>
    <a href="https://chenwangjeremy.net/">
      <img src="https://chenwangjeremy.net/assets/img/profile.jpeg">
      <div>Chen Wang</div>
      <div class="aff">Stanford</div>
    </a>
    <a href="https://taochenshh.github.io/">
      <img src="https://taochenshh.github.io/media/tao_profile.jpg">
      <div>Tao Chen</div>
      <div class="aff">MIT</div>
    </a>
    <a href="https://contactrika.github.io/">
      <img src="images/rika_photo_one.jpg">
      <div>Rika Antonova</div>
      <div class="aff">University of Cambridge</div>
    </a>

    </div>
    <div class="people">
      <a href="https://yzqin.github.io/">
        <img src="https://yzqin.github.io/file/qyz_circle.png">
        <div>Yuzhe Qin</div>
        <div class="aff">UC San Diego</div>
      </a>
      <a href="https://www.lenroe.com/">
        <img src="images/lennart.jpeg">
        <div>Lennart RÃ¶stel</div>
        <div class="aff">TUM</div>
      </a>
      <a href="https://aidx-lab.org/manipulation/">
        <img src="images/johannes.jpeg">
        <div>Johannes Pitz</div>
        <div class="aff">TUM</div>
      </a>
      <a href="https://3d.snu.ac.kr/">
        <img src="images/youngmin_kim.jpg">
        <div>Young Min Kim</div>
        <div class="aff">SNU</div>
      </a>
    </div>
  </div>

  <br>
  <a href="https://www.epfl.ch/labs/lasa/workshop-fine-manipulation/">Link</a> to the other workshop webpage.

  <div class="section" id="contact">
    <h2>Contact</h2>
    <div>For questions and comments, please <a href="mailto:hqi@berkeley.edu">contact us.</a>
    </div>
  </div>

  <div class="foot">
    Â© CoRL 2024 Learning Dexterous Manipulation Workshop
  </div>
</div>


</body>

</html>